{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBD_Runs/1\n",
      "Directory Already Exists\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Operating System\n",
    "import os\n",
    "\n",
    "# Convenience\n",
    "from tqdm import tqdm\n",
    "# import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.max_columns', 250)\n",
    "\n",
    "from medicalbiasdetection import utils\n",
    "\n",
    "# Global Variables\n",
    "RUN = 1\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# setup configuration files\n",
    "config = utils.read_yaml()\n",
    "config_preprocessing = utils.read_yaml('conf/mbd_run_log.yaml')\n",
    "\n",
    "# Create Run Directory\n",
    "utils.create_run_dir(str(RUN))\n",
    "\n",
    "LOG_DIR = config['LOG']['dir'].format(RUN=RUN)\n",
    "LOG_PATH = config['LOG']['path'].format(RUN=RUN)\n",
    "os.environ['LOG_PATH'] = LOG_PATH\n",
    "os.environ['RUN'] = str(RUN)\n",
    "\n",
    "\n",
    "from medicalbiasdetection import process, cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encounters (csn): 119733\n",
      "Years: [2016 2017 2018 2019 2020]\n",
      "Start year: 2016\n",
      "End year: 2020\n",
      "Number of unique patient visits: 119733\n",
      "Number of unique patients: 73484\n",
      "Number of sepsis=0 patients: 101269 (84.58%)\n",
      "Number of sepsis=1 patients: 18464 (15.42%)\n"
     ]
    }
   ],
   "source": [
    "# identify the medical facility for the dataset\n",
    "med_fac = 'grady' # 'grady' # 'emory'\n",
    "X = cohort.load_reference_data(med_fac,config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset CSN Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file created at: \n",
      " MBD_Runs/1/data/log/csn_log.csv\n"
     ]
    }
   ],
   "source": [
    "run_cell = True\n",
    "if run_cell:\n",
    "    cohort.create_csn_log(X, LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file updated.\n",
      "Number of encounters (csn): 17798\n",
      "Years: [2016 2017 2018 2019 2020]\n",
      "Start year: 2016\n",
      "End year: 2020\n",
      "Number of unique patient visits: 17798\n",
      "Number of unique patients: 16178\n",
      "Number of sepsis=0 patients: 10538 (59.21%)\n",
      "Number of sepsis=1 patients: 7260 (40.79%)\n",
      "Number of encounters (csn): 17798\n",
      "Years: [2016 2017 2018 2019 2020]\n",
      "Start year: 2016\n",
      "End year: 2020\n",
      "Number of unique patient visits: 17798\n",
      "Number of unique patients: 16178\n",
      "Number of sepsis=0 patients: 10538 (59.21%)\n",
      "Number of sepsis=1 patients: 7260 (40.79%)\n",
      "Log file updated.\n",
      "Number of encounters (csn): 15182\n",
      "Years: [2016 2017 2018 2019 2020]\n",
      "Start year: 2016\n",
      "End year: 2020\n",
      "Number of unique patient visits: 15182\n",
      "Number of unique patients: 13917\n",
      "Number of sepsis=1 patients: 6662 (43.88%)\n",
      "Number of sepsis=0 patients: 8520 (56.12%)\n",
      "Log file updated.\n",
      "Number of encounters (csn): 15179\n",
      "Years: [2016 2017 2018 2019 2020]\n",
      "Start year: 2016\n",
      "End year: 2020\n",
      "Number of unique patient visits: 15179\n",
      "Number of unique patients: 13914\n",
      "Number of sepsis=1 patients: 6662 (43.89%)\n",
      "Number of sepsis=0 patients: 8517 (56.11%)\n",
      "Original Size: 119733\n",
      "Reduced Size: 15179\n",
      "Data Reduction: 104554\n"
     ]
    }
   ],
   "source": [
    "# copy static dataset\n",
    "X_proc = X.copy()\n",
    "n_orig = len(X_proc)\n",
    "\n",
    "# set step variable\n",
    "step = 'preprocessing'\n",
    "\n",
    "# remove non-ICU patients\n",
    "drop_csns = X_proc[X_proc['first_icu_start'].isna()]['csn'].unique().tolist()\n",
    "X_proc = cohort.remove_csn(X_proc, drop_csns, step,'non-ICU patient')\n",
    "# print csn report\n",
    "cohort.print_cohort_report(X_proc,'csn','pat_id','sepsis')\n",
    "\n",
    "# remove patients under the age of 18\n",
    "drop_csns = X_proc[X_proc['age']<18]['csn'].unique().tolist()\n",
    "X_proc = cohort.remove_csn(X_proc, drop_csns, step,'under 18 years of age')\n",
    "# print csn report\n",
    "cohort.print_cohort_report(X_proc,'csn','pat_id','sepsis')\n",
    "\n",
    "\n",
    "# remove csns with less than 24 hours of data\n",
    "drop_csns = X_proc[X_proc['hoursICU']<24]['csn'].unique().tolist()\n",
    "X_proc = cohort.remove_csn(X_proc, drop_csns, step, 'less than 24 hours of icu data')\n",
    "# print csn report\n",
    "cohort.print_cohort_report(X_proc,'csn','pat_id','sepsis')\n",
    "\n",
    "# remove csns with unknown gender\n",
    "# drop_csns = X_proc[X_proc['gender']>1]['csn'].unique().tolist()\n",
    "drop_csns = X_proc[(X_proc['gender']!=0)&(X_proc['gender']!=1)]['csn'].unique().tolist()\n",
    "X_proc = cohort.remove_csn(X_proc, drop_csns, step, 'gender unknown')\n",
    "# print csn report\n",
    "cohort.print_cohort_report(X_proc,'csn','pat_id','sepsis')\n",
    "\n",
    "n_reduced = len(X_proc)\n",
    "print(f\"Original Size: {n_orig}\")\n",
    "print(f\"Reduced Size: {n_reduced}\")\n",
    "print(f\"Data Reduction: {n_orig - n_reduced}\")\n",
    "\n",
    "X = X_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CSN logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSNs included: 15179\n",
      "Removed CSNs:\n",
      "step           reason                        \n",
      "preprocessing  non-ICU patient                   101935\n",
      "               less than 24 hours of icu data      2616\n",
      "               gender unknown                         3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = cohort.update_cohort(X,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Data CSV 0 Complete\n",
      "Starting Patient Data CSV 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [24:43<1:38:52, 1483.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Data CSV 1 Complete\n",
      "Starting Patient Data CSV 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [50:15<1:15:37, 1512.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Data CSV 2 Complete\n",
      "Starting Patient Data CSV 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [1:14:17<49:19, 1479.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Data CSV 3 Complete\n",
      "Starting Patient Data CSV 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:44:46<26:57, 1617.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Data CSV 4 Complete\n",
      "Starting Patient Data CSV 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:58:54<00:00, 1426.98s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'medicalbiasdetection.process' has no attribute 'remove_csn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b7644bf566fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mskipped_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mdrop_csns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskipped_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_csn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_csns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'corrupted file - did not process'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'medicalbiasdetection.process' has no attribute 'remove_csn'"
     ]
    }
   ],
   "source": [
    "run_cell = True\n",
    "debug = False\n",
    "\n",
    "if run_cell:\n",
    "    \n",
    "    # establish parameters\n",
    "    prediction_lags = [6]\n",
    "\n",
    "    # set window size\n",
    "    window = 6\n",
    "\n",
    "    # get pickle dtype map\n",
    "    path = config['data']['pkl_dtypes']\n",
    "    with open(path,\"r\") as file:\n",
    "        pkl_map = json.load(file)\n",
    "\n",
    "    # collect important column names\n",
    "    imp1_cols = [x[0] for x in lab_trt_dict.items() if (x[1]['importance']=='1')] + ['sbp_line','dbp_line','map_line']\n",
    "    \n",
    "    # get feature informative missing columns\n",
    "    fim_cols = config['preprocess']['fim_cols']\n",
    "    \n",
    "    #  get vital sign columns\n",
    "    vital_cols = config['preprocess']['vital_cols']\n",
    "\n",
    "    # get high missing variables to remove\n",
    "    drop_missing = config['preprocess']['drop_missing']\n",
    "    \n",
    "    # get remaining variables to remove\n",
    "    drop_cols = config['preprocess']['drop_cols']\n",
    "    \n",
    "    # get list of years to increment for loop\n",
    "    years = np.sort(X.year.unique()).tolist()\n",
    "\n",
    "    # initialize counters\n",
    "    n=0\n",
    "    save_i = 0\n",
    "\n",
    "    # loop through each year\n",
    "    for year in (tqdm(years,leave=True)):\n",
    "\n",
    "        # filter data by year\n",
    "        data = X[X.year==year]\n",
    "\n",
    "        # loop through each patient \n",
    "        for ind,row in data.iterrows():\n",
    "\n",
    "            # patient csn\n",
    "            csn = row.csn\n",
    "\n",
    "            # first hour of icu status\n",
    "            icu_start_time = row.first_icu_start\n",
    "\n",
    "            # first hour of hospital admission\n",
    "            hosp_admit = row.start_index\n",
    "\n",
    "            # sepsis (bool)\n",
    "            sepsis = row.sepsis\n",
    "\n",
    "            # sofa2 (bool)\n",
    "            sofa_2 = row.sofa_2\n",
    "\n",
    "            # set values for sofa and sepsis times\n",
    "            sofa_2_time = None\n",
    "            sep_time = None\n",
    "\n",
    "            try:\n",
    "                # create path to patient ehr data\n",
    "                path = config['data']['pat_pkl'].format(year=year,csn=csn)\n",
    "\n",
    "                # ingest patient data file\n",
    "                p_pkl = pd.read_pickle(path) \n",
    "\n",
    "                # get super_table (time-series) EMR data\n",
    "                hosp_data = p_pkl[\"super_table\"].copy()\n",
    "\n",
    "                # assign data types to each column\n",
    "                hosp_data = hosp_data.astype(pkl_map,errors='ignore')\n",
    "\n",
    "                # keep columns of importance 1\n",
    "                hosp_data = hosp_data[imp1_cols]\n",
    "\n",
    "                # assign csn\n",
    "                hosp_data['csn'] = csn\n",
    "\n",
    "                # create sepsis labels, assign all equal to 0\n",
    "                hosp_data['sepsis'] = 0\n",
    "\n",
    "                # set lagged sepsis values to 0\n",
    "                for lag in prediction_lags: \n",
    "                    hosp_data[f'sepsis_lag_{lag}'] = 0\n",
    "\n",
    "                # create sofa score time, assign time labels to 0\n",
    "                hosp_data['sofa_2_time'] = 0\n",
    "\n",
    "                # get sofa and sirs score totals\n",
    "                hosp_data['sofa_score_total']= p_pkl['sofa_scores']['hourly_total_mod'].fillna(0.0).tolist()\n",
    "                hosp_data['sirs_score_total']= p_pkl['sirs_scores']['hourly_total'].fillna(0.0).tolist()\n",
    "\n",
    "                # if the patient was identified as having sepsis during their ICU time\n",
    "                if sepsis: \n",
    "                    # set sepsis time\n",
    "                    sep_time = row.first_sep3_time\n",
    "\n",
    "                    # adjust the sepsis time if it is not a time in the index\n",
    "                    if sep_time not in hosp_data.index:\n",
    "\n",
    "                        # find the next closest time slot available if the sepsis time is not in the patient's indexed times\n",
    "                        sep_time = process.nextClosestTime(hosp_data.index,sep_time)\n",
    "\n",
    "                    # set sepsis time to 1 beginning from first sepsis hour to the end of patient's stay\n",
    "                    hosp_data.loc[sep_time:,'sepsis'] = 1\n",
    "\n",
    "                    # set sepsis lags respectively\n",
    "                    for lag in prediction_lags:\n",
    "                        hosp_data[f'sepsis_lag_{lag}'] = hosp_data['sepsis'].shift(-lag).ffill()\n",
    "                    if debug:\n",
    "                        print(\"Lags updated\")\n",
    "\n",
    "                # if patient has a sofa score, create sofa_2 labels\n",
    "                if sofa_2: \n",
    "                    # set patient sofa 2 time\n",
    "                    sofa_2_time = row.sofa_2_time\n",
    "\n",
    "                    # adjust the sofa 2 time if it is not a time in the index\n",
    "                    if sofa_2_time not in hosp_data.index:\n",
    "                        # find the next closest time slot available if the sofa 2 time is not in the patient's indexed times\n",
    "                        sofa_2_time = process.nextClosestTime(hosp_data.index,sofa_2_time)\n",
    "\n",
    "                    # set sofa 2 time to 1 beginning from first sofa time to the end of patient's stay\n",
    "                    hosp_data.loc[sofa_2_time:,'sofa_2_time'] = 1\n",
    "\n",
    "                    if debug:\n",
    "                        print(\"Sofa 2 Information updated\")\n",
    "\n",
    "                # shift df to icu start time to remove information collected outside of the icu\n",
    "                hosp_data = hosp_data.loc[icu_start_time:]\n",
    "\n",
    "                # copy timestamps from index to col times\n",
    "                hosp_data['timestamp'] = hosp_data.index\n",
    "\n",
    "                # forward fill feature informative missingness columns\n",
    "                hosp_data[imp1_cols] = hosp_data[imp1_cols].ffill()\n",
    "\n",
    "                #Add feature informative missining columns\n",
    "                feat_info_missing_df = process.feature_informative_missingness(hosp_data[fim_cols])\n",
    "\n",
    "                # combine fim data and hospital data\n",
    "                hosp_data = hosp_data.merge(feat_info_missing_df, on=fim_cols)\n",
    "\n",
    "                # remove heavily missing variates\n",
    "                hosp_data = hosp_data.drop(columns=drop_missing, errors='ignore')\n",
    "\n",
    "                # drop duplicates\n",
    "                hosp_data = hosp_data.drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "                # sort data based on time\n",
    "                hosp_data = hosp_data.sort_values(by=['timestamp'])\n",
    "\n",
    "                # reset the index\n",
    "                hosp_data = hosp_data.reset_index()\n",
    "\n",
    "                #Add feature sliding window data\n",
    "                hosp_data = pd.concat([hosp_data, process.feature_slide_window(hosp_data[vital_cols],window)], axis=1)\n",
    "\n",
    "                # Add feature empiric scores\n",
    "                hosp_data = process.feature_empiric_score(hosp_data)\n",
    "\n",
    "                # replace current index with patient index id and icu hour \"0_0\" = patient index 0 at hour 0\n",
    "                hosp_data.index = [f\"{n}_{x}\" for x in range(len(hosp_data))]\n",
    "\n",
    "                # reduce dataset to specified hour upper limits\n",
    "                hosp_data = hosp_data.iloc[:168]\n",
    "                \n",
    "                # drop remaining unnecessary variables\n",
    "                hosp_data = hosp_data.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "                # save data after each set of 3000 patients\n",
    "                if n%3000 == 0:\n",
    "                    print(f\"Patient Data CSV {save_i} Complete\")\n",
    "                    save_i += 1\n",
    "                    print(f\"Starting Patient Data CSV {save_i}\")\n",
    "\n",
    "                # set data type for saving\n",
    "                TYPE = 'hourly'\n",
    "                # set directory path\n",
    "                outpath_dir = config['DIR']['data'].format(RUN=RUN, TYPE=TYPE)\n",
    "                # set filename\n",
    "                filename = f\"processed_data_{save_i}.csv\"\n",
    "                # create save path\n",
    "                outpath = os.path.join(outpath_dir,filename)\n",
    "\n",
    "                if not debug:\n",
    "                    hosp_data.to_csv(outpath, mode='a', header=not os.path.exists(outpath))\n",
    "\n",
    "                # increment patient index counter\n",
    "                n+=1\n",
    "\n",
    "                if debug:\n",
    "                    stop =2\n",
    "                    if n >= stop:\n",
    "                        break\n",
    "\n",
    "            except:\n",
    "                # set data type for saving\n",
    "                TYPE = 'skipped'\n",
    "\n",
    "                # set directory path\n",
    "                outpath_dir = config['DIR']['data'].format(RUN=RUN, TYPE=TYPE)\n",
    "\n",
    "                # set filename\n",
    "                filename =f\"processed_data_skipped.csv\"\n",
    "\n",
    "                # create save path\n",
    "                skip_outpath = os.path.join(outpath_dir,filename)\n",
    "\n",
    "                skipped = pd.DataFrame([csn,year]).T\n",
    "\n",
    "                skipped.to_csv(skip_outpath, mode='a', header=not os.path.exists(skip_outpath), index= False)\n",
    "    \n",
    "    # Update the cns log\n",
    "    TYPE='skipped'\n",
    "    # set directory path\n",
    "    outpath_dir = config['DIR']['data'].format(RUN=RUN, TYPE=TYPE)\n",
    "\n",
    "    # set filename\n",
    "    filename =f\"processed_data_skipped.csv\"\n",
    "\n",
    "    path = os.path.join(outpath_dir,filename)\n",
    "    skipped_df = pd.read_csv(path)\n",
    "    drop_csns = skipped_df.iloc[:,0].unique().tolist()\n",
    "    _ = cohort.remove_csn(X, drop_csns, step, 'corrupted file - did not process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CSN logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSNs included: 13289\n",
      "Removed CSNs:\n",
      "step           reason                          \n",
      "preprocessing  non-ICU patient                     101935\n",
      "               less than 24 hours of icu data        2616\n",
      "               corrupted file - did not process      1890\n",
      "               gender unknown                           3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "_ = cohort.update_cohort(X,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
